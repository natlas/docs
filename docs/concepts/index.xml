<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natlas â€“ Concepts</title><link>/docs/concepts/</link><description>Recent content in Concepts on Natlas</description><generator>Hugo -- gohugo.io</generator><atom:link href="/docs/concepts/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Host Coverage Strategy</title><link>/docs/concepts/host_coverage_strategy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/concepts/host_coverage_strategy/</guid><description>
&lt;p>The Natlas platform is a system capable of running either for long-term inventory scans with historical records or as a system intended for use as a search-head during an assessment. Natlas is intended for scanning any range scope - from a small local network up to the entire internet.&lt;/p>
&lt;p>Therefore scanning in Natlas has some operational requirements:&lt;/p>
&lt;ul>
&lt;li>It can not stress network bottlenecks
&lt;ul>
&lt;li>This can happen if scans for different targets share networking routes within short durations (e.g. anycast)&lt;/li>
&lt;li>Stability is critical for getting reliable and trustworthy results&lt;/li>
&lt;li>Avoids needlessly triggering alarms&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>It must avoid interrogation by monitoring strategies
&lt;ul>
&lt;li>Being detected by monitoring is typically Not-A-Big-Deal (TM), however nice to avoid&lt;/li>
&lt;li>It&amp;rsquo;s more important to have the scan targets be difficult to correlate
&lt;ul>
&lt;li>Difficult to infer scope and objectives of the campaign&lt;/li>
&lt;li>Difficult to relate scans to later stages&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Low resource cost on behalf of the scanner (e.g. CPU, memory)&lt;/li>
&lt;/ul>
&lt;p>For tools that are either long-running or are critical applications, other features may be desirable, for example:&lt;/p>
&lt;ul>
&lt;li>Provide a guarantee that coverage has been achieved&lt;/li>
&lt;li>Resumable/interruptible scans, or scans that can be migrated&lt;/li>
&lt;li>Scope that can be updated during runtime to meet pacing needs of the operating group&lt;/li>
&lt;/ul>
&lt;p>Because Natlas is used to provide comprehensive views of network inventory, guaranteeing coverage after a given amount of time is a high priority. It&amp;rsquo;s also useful to Natlas for scans to have a level of determinism - so that scans (including those deployed for large scopes) can be continued if the systems involved crash or are otherwise interrupted.&lt;/p>
&lt;h2 id="strategy">Strategy&lt;/h2>
&lt;p>To do this, the scanning engine performs randomized scans. Randomized scans avoid visiting clusters of similar target addresses (which could be anycast to the same systems, or otherwise rely on similar intermediary nodes and networking paths). This makes bursts of activity in any particular network route statistically unlikely by spreading out target traffic across the entire scope.&lt;/p>
&lt;p>Calling into an operating system provided source of randomness is a typical way to solve this problem. However, uniformly sampling scope ranges without storing state about previous scans and compensating for them makes it statistically improbable that &lt;strong>N&lt;/strong> randomly chosen network scans targeting &lt;strong>N&lt;/strong> addresses will result in full coverage. In fact, after one round of &lt;strong>N&lt;/strong> random samples only slightly more than half of endpoints are expected to have been scanned, and the scanner will have to scan about &lt;strong>12x-24x&lt;/strong> as many to expect to get full coverage (even then there are no guarantees, and the situation is worse for the larger ranges concerned in IPv6).&lt;/p>
&lt;p>Storing some kind of state to target new scans toward unreached endpoints seems inevitable. Unfortunately, storing full tables of scan state information for all endpoints is a formidable memory management problem for scans that grow larger than hundreds of thousands of endpoints (especially for full internet scans, or scans over IPv6 space). Traditional data-structures like bloom filters, cuckoo filters, and MRE caches have disadvantages that they speed up scan coverage without providing full coverage guarantees at the addititional cost of introducing false positives.&lt;/p>
&lt;p>Natlas uses a mathematical structure called a cyclic group to provide well distributed random numbers that also provably fully cover &lt;strong>N&lt;/strong> targets with &lt;strong>N&lt;/strong> sample scans. While generic versions of the algorithms involved with computing cyclic groups are complex and large sizes are used in modern cryptography, Natlas uses small cyclic groups whose computation is well within reasonable expected runtimes.&lt;/p>
&lt;h2 id="mathematical-background">Mathematical Background&lt;/h2>
&lt;p>There&amp;rsquo;s a large amount of background and terminology that this article will not go into (sorry number theorists!) out of respect for time.&lt;/p>
&lt;blockquote>
&lt;p>For number theorists - in short: Natlas uses a multiplicative cyclic group of prime order, and computes the factorization of the order of the group to obtain possible subgroup orders. Natlas uses these subgroup orders for generator testing, and finds random generators. Natlas uses these generators to determine the order of the scan. Mapping between group elements and IP addresses is performed by creating an index over IP addresses using canonical IP ordering.&lt;/p>
&lt;/blockquote>
&lt;p>Wikipedia describes Cyclic Groups as &amp;ldquo;In algebra, a cyclic group or monogenous group is a group that is generated by a single element.&amp;rdquo; This description is abstract but alludes the mathematical fact that we exploit for scanning in Natlas: an entire &amp;ldquo;group&amp;rdquo; (of IP addresses) can be generated by a small amount of state/information, and we&amp;rsquo;re guaranteed to generate every IP address in some order (we&amp;rsquo;ll later show how that order is pseudorandom).&lt;/p>
&lt;p>Let&amp;rsquo;s try out generating the &amp;ldquo;group of numbers 1-6&amp;rdquo; using the &amp;ldquo;single element 3&amp;rdquo;. We will multiply 3 by itself over and over, and take the remainder &lt;strong>mod 7&lt;/strong> (to keep the values between 1 and 6).&lt;/p>
&lt;p>3 mod 7 = &lt;strong>3&lt;/strong>&lt;/p>
&lt;p>3x3 mod 7 = &lt;strong>2&lt;/strong>&lt;/p>
&lt;p>3x3x3 mod 7 = &lt;strong>6&lt;/strong>&lt;/p>
&lt;p>3x3x3x3 mod 7 = &lt;strong>4&lt;/strong>&lt;/p>
&lt;p>3x3x3x3x3 mod 7 = &lt;strong>5&lt;/strong>&lt;/p>
&lt;p>3x3x3x3x3x3 mod 7 = &lt;strong>1&lt;/strong>&lt;/p>
&lt;p>By using &amp;ldquo;3&amp;rdquo; as a generator, we were able to obtain all numbers 1-6 in a random-looking order. It&amp;rsquo;s important that &amp;ldquo;mod 7&amp;rdquo; is using a prime number, or else this won&amp;rsquo;t work.&lt;/p>
&lt;blockquote>
&lt;p>Mathematicians have proven that every cyclic group that isn&amp;rsquo;t infinite in size, if the items in the &amp;ldquo;group&amp;rdquo; are relabeled, is equivalent to the numbers &lt;strong>mod n&lt;/strong> where &lt;strong>n&lt;/strong> is a prime number or a prime number raised to a power.&lt;/p>
&lt;/blockquote>
&lt;p>What Natlas will do is determine the total size of a scan scope, say 12345678 total IP addresses, and find the next prime number 12345701. Using calculations &amp;ldquo;mod 12345701&amp;rdquo;, we find a generator (the number 3 in the above example) and use it to list every number from 1-12345700 in a random-looking order. Natlas needs to throw away every number between 12345678 and 12345700, but there aren&amp;rsquo;t that many of them.&lt;/p>
&lt;p>There are important implementation details.&lt;/p>
&lt;h3 id="finding-a-generator">Finding a Generator&lt;/h3>
&lt;p>Not every number is a generator for a cyclic group. Take for example the proposed generator 2 with the group 1-6 from above:&lt;/p>
&lt;p>2 mod 7 = &lt;strong>2&lt;/strong>&lt;/p>
&lt;p>2x2 mod 7 = &lt;strong>4&lt;/strong>&lt;/p>
&lt;p>2x2x2 mod 7 = &lt;strong>1&lt;/strong>&lt;/p>
&lt;p>The next number generated (2x2x2x2 mod 7 = 2) is one that we&amp;rsquo;ve already visited. The reason for this is that 2 is a generator for a subgroup, and not the whole group. You may notice that the subgroup generated is exactly half as long as the whole group. That&amp;rsquo;s a good observation, and its true that all subgroups have a length that evenly divides the total size of the whole group.&lt;/p>
&lt;p>Natlas needs to find a true generator. To do this, it takes the prime number for the group, and subtracts one to get the number of values in (or &amp;ldquo;order of&amp;rdquo;) the group. As an example, &amp;ldquo;mod 7&amp;rdquo; has 6 elements. Then, the scanning engine factors this number to obtain all possible subgroup lengths. It randomly generates possible generators and tests them to see if they generate any subgroups. If a number is not a generator for any subgroup, it generates the whole group!&lt;/p>
&lt;p>The number of generators for a given group can be counted using a complex formula (involving the number of relatively prime numbers to other numbers). Needless to say, there are always a &amp;ldquo;lot&amp;rdquo; of generators, and this method of generation will quickly find one.&lt;/p>
&lt;p>Our scanning engine chooses a random generator every restart, and picks a random location in that generator&amp;rsquo;s cycle to start scanning from. This increases the uniform pseudorandom nature of the scanning strategy.&lt;/p>
&lt;h3 id="computing-modular-exponentiation-quickly">Computing Modular Exponentiation Quickly&lt;/h3>
&lt;p>The scanning engine uses an algorithm capable of quickly calculating exponents, and keeps the numbers small by keeping all intermediate values modulo the group order.&lt;/p>
&lt;p>Essentially, 3x3x3x3x3 mod 7 doesn&amp;rsquo;t need to be explicitly calculated from scratch if 3x3x3x3 mod 7 already has. Just multiply the second number by the 3 to obtain the first.&lt;/p>
&lt;p>Calculating arbitrarily large exponents of arbitrary generators can be done by repeatedly squaring an intermediate value, and multiplying that value into a final calculation if it contributes.&lt;/p>
&lt;p>Wikipedia hosts a well-written article on the details of &lt;a href="https://en.wikipedia.org/wiki/Exponentiation_by_squaring">Exponentiation by squaring&lt;/a>&lt;/p>
&lt;h3 id="indexing-ip-addresses">Indexing IP Addresses&lt;/h3>
&lt;p>When all of the elements of the group can be generated, we need to be able to translate them back into IP addresses. To do this, we put IP network blocks into a random access array in canonical address order (e.g. 1.1.1.1 &amp;lt; 192.168.0.1). One time at the start of any scan configuration, the scanning engine walks the address blocks and counts the total number of IP addresses accumulated up until each networking block.&lt;/p>
&lt;p>With an index, binary search can then be used to determine which network address range is associated, and within that range, which IP address is mapped. It&amp;rsquo;s important to note that changing the scope of a scan causes all of the addresses to be reindexed.&lt;/p>
&lt;h3 id="but-factoring-and-discrete-log-are-hard">But Factoring and Discrete Log Are Hard&lt;/h3>
&lt;p>Yes and No!&lt;/p>
&lt;p>Natlas&amp;rsquo;s generator has some properties which make it similar to provably secure pseudorandom number generators (e.g. Blum Blum Shub), but the numbers involved - 32 bits for IPv4 128 bits for IPv6 - are small enough that these calculations can be done rather simply. Performance has been tested on the full 32 bit range and into the 64 bit ranges without any obvious performance issues.&lt;/p>
&lt;h3 id="the-average-time-delta-is-too-variable">The Average Time Delta is Too Variable&lt;/h3>
&lt;p>If you&amp;rsquo;re trying to get consistent scan cycles, it may be valuable to not generate a new random order for every cycle. The random order tends to mean that sometimes a host is scanned a couple minutes apart and other times a couple days apart, which isn&amp;rsquo;t necessarily ideal. To this end, Natlas introduced a &lt;code>CONSISTENT_SCAN_CYCLE&lt;/code> option that can be set, which will reuse the same random order as long as the scope hasn&amp;rsquo;t changed, resulting in more consistent timing between scans of a given host.&lt;/p>
&lt;h2 id="show-me-the-code">Show me the code&lt;/h2>
&lt;p>The &lt;a href="https://github.com/natlas/cyclicprng">cyclic PRNG&lt;/a> has been separated out from the Natlas platform to a standalone package that can be installed via pip. You can checkout the code yourself or jump right in by running &lt;code>pipenv install cyclicprng&lt;/code>.&lt;/p>
&lt;p>Initialization of the PRNG is quick, even for address spaces up to 2^128 (the entire IPv6 address space) - typically less than a tenth of a second on a modern computer.&lt;/p>
&lt;p>The process by which these cyclical random numbers are turned into an IP address to scan has not been separated from the Natlas server at this time, however the logic is pretty well consolidated into &lt;a href="https://github.com/natlas/natlas/blob/main/natlas-server/app/scope/scan_manager.py">the scan manager&lt;/a>.&lt;/p></description></item><item><title>Docs: Port Coverage Strategies</title><link>/docs/concepts/port_coverage_strategies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/concepts/port_coverage_strategies/</guid><description>
&lt;h2 id="current-strategy">Current Strategy&lt;/h2>
&lt;p>Currently, we have only one port coverage strategy - Define ports to be given to the natlas-agent via the natlas-services file. This feeds directly to nmap and we scan all the ports in the natlas-services file. This has the advantage of being simple while also allowing per-deployment customization of what ports to scan. But it has the disadvantage of not scaling up very well; you could add another 5000 ports but nmap is not a very fast scanner by default. So basically you don&amp;rsquo;t identify anything &lt;em>new&lt;/em> with this approach, only the things that you think you might find ahead of time.&lt;/p>
&lt;h2 id="proposals">Proposals&lt;/h2>
&lt;p>Ideally we&amp;rsquo;d like to have all of these capabilities to allow for flexibility in configuration, as each has it&amp;rsquo;s own drawbacks and advantages.&lt;/p>
&lt;h3 id="the-multi-armed-bandit">The Multi Armed Bandit&lt;/h3>
&lt;p>It would be possible to design scanning behavior that models the scarceness of scanning resources and optimizes its use, given a given scanning scope. Since every scan costs CPU and networking resources as well as latency, and any scan on any port has some statistical chance at reward that depends on the port, Natlas could model the scanning problem as an instance of the well studied &lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-Armed Bandit problem&lt;/a> for which optimal and rapidly converging strategies are known.&lt;/p>
&lt;p>Effectively, when it is time to scan a particular host - or set of hosts - the Natlas server would utilize information it has previously collected to fine tune its scanning ranges. In a &amp;ldquo;vanilla&amp;rdquo; multi-armed bandit model, the decision of the server to scan hosts on specific ports would only include information about which ports have been seen before as statistical aggregates across the entire scope. However, if a variations of the problem - such as &amp;ldquo;contextual bandits&amp;rdquo; - were used as the model, it would be possible to combine scope tags, ASNs, and/or other information with port statistics to narrow in on favorable port scan targets.&lt;/p>
&lt;p>The resulting experience would be that a Natlas administrator would set a certain number of ports each agent would be expected to scan, but not specify which ports these should be or could optionally provide &amp;ldquo;seed ports&amp;rdquo; that are initial exposure guesses to start the scan off with. Natlas would then - over the course of several scanning Epochs - use scan results it is collecting as feedback to refine what ports it is asking the agents to scan. Effectively, over time, this would result in scanning the most common ports in &lt;em>that specific&lt;/em> scope, without needing to scan all 65k ports on all hosts all the time.&lt;/p>
&lt;p>The algorithms that solve this problem always allocate a few resources for &amp;ldquo;exporation&amp;rdquo;, which means that even after a large number of scanning epochs has passed, new scans will balance the risk of scanning ports not likely to result in discoveries against the need to determine probe unlikely ports to learn whether historically collected statistics are still accurate.&lt;/p>
&lt;p>&lt;strong>Pros:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Results in an efficient use of scans&lt;/li>
&lt;li>Works in scenarios where few common ports are active, and they may not be known in advance&lt;/li>
&lt;li>Can be tuned to be more aggressive or conservative and to learn faster or slower&lt;/li>
&lt;li>Can start out identical to default port lists (e.g. nmap)&lt;/li>
&lt;li>Can be combined with other approaches so that there are a &amp;ldquo;must have&amp;rdquo; port scan, and remaining ports are &amp;ldquo;learned&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Cons:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Can not guarantee that specific hosts/port combinations will be scanned&lt;/li>
&lt;li>Host history will not be garunteed to be an apples-to-apples comparison&lt;/li>
&lt;li>This introduces state into the server portion of Natlas&lt;/li>
&lt;li>The architecture would need to be redesigned to support dynamic per-scan port and service targets&lt;/li>
&lt;li>Bugs in scanning logic or deployment will result in artifacts that get &amp;ldquo;learned&amp;rdquo;. For example, if the scanner loses all packets inbound from the target scope, the strategy will learn that all ports are equally as good (since scan results indicate that all ports are down for all hosts)&lt;/li>
&lt;/ul>
&lt;h3 id="pre-flight-masscan">Pre-flight Masscan&lt;/h3>
&lt;p>This is a simple proposal for an agent to run masscan for all ports against each host before passing the open ports to nmap for deeper scanning. At a rate limit of 10000pps, this would add approximately 65 seconds per agent job. It would take some time out of the nmap scanning phase, because nmap would receive a list of ports that should be open, so it shouldn&amp;rsquo;t be spending much time scanning ports that aren&amp;rsquo;t open. It would also have the benefit of not relying on nmap&amp;rsquo;s host discovery, which can often be tricked into reporting a host as down. The option currently exists to tell nmap to skip it&amp;rsquo;s host discovery phase (&lt;code>-Pn&lt;/code>), but this is also slow because it means it checks every port no matter what. It is quite common across recon enthusiasts to use masscan prior to nmap, though the typical model for masscan is a port-oriented strategy which scans large ranges of network space for a limited number of ports, rather than what we&amp;rsquo;re doing here.&lt;/p></description></item><item><title>Docs: Scope Ingestion Strategies</title><link>/docs/concepts/scope_ingestion_strategies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/concepts/scope_ingestion_strategies/</guid><description>
&lt;h2 id="current-strategy">Current Strategy&lt;/h2>
&lt;p>Currently, Natlas only supports manually imported scope in the form of CIDR ranges. This may work well for scans of RFC1918 address space, as well as for larger organizations that have static CIDR ranges on the internet. But it really doesn&amp;rsquo;t work well for smaller organizations using cobbled together address space, let alone cloud-native companies.&lt;/p>
&lt;h2 id="possible-strategies">Possible Strategies&lt;/h2>
&lt;ul>
&lt;li>(Priority) DNS enumeration ingestion
&lt;ul>
&lt;li>Given one or more apex domains, perform subdomain enumeration and use valid subdomains to populate scope.&lt;/li>
&lt;li>Requires the ability to:
&lt;ol>
&lt;li>Automatically add ip addresses of new subdomains to scanning&lt;/li>
&lt;li>Add new subdomains to a review queue where an administrator decides to scan or not&lt;/li>
&lt;li>Blacklist DNS names such that no matter what ip they point to, we do not scan them&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Can be automated fairly well with the use of amass for enumeration and background workers that regularly re-resolve already known records (and keep a history of these re-resolves)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Authenticated DNS zone transfer ingestion
&lt;ul>
&lt;li>If you are an administrator or on the IT team for an organization, you may wish to perform authenticated zone transfers in order to maintain an authoritative source of DNS records to scan for&lt;/li>
&lt;li>Likely a relatively low priority option, and could be easily automated with a Natlas API and a curl script in the meantime&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Cloud Provider Events API(s)
&lt;ul>
&lt;li>Receive / subscribe to events for your cloud provider in order to receive notification of new hosts being deployed and then automatically feed these into natlas scope&lt;/li>
&lt;li>Works really well for scanning primarily cloud environments&lt;/li>
&lt;li>Likely requires plugins for GCP, AWS, Azure, etc&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Cloud Provider Enumeration API(s)
&lt;ul>
&lt;li>Periodically (on a configurable time) run tooling that queries cloud provider APIs to build a list of hosts to be ingested for scanning&lt;/li>
&lt;li>Can create a minor gap in receiving information about new hosts when compared to the previous events api source&lt;/li>
&lt;li>Similarly will require plugins for GCP, AWS, Azure, etc&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="scope-ingestion-methods">Scope Ingestion Methods&lt;/h2>
&lt;ul>
&lt;li>Add or remove a single item from scope/blacklist&lt;/li>
&lt;li>Add or remove a batch of items from scope/blacklist&lt;/li>
&lt;li>Replace the entire scope/blacklist for a given ingestion source
&lt;ul>
&lt;li>We don&amp;rsquo;t want DNS zone transfer updates to be additive, for instance. We know these will be the hosts to scan and will likely want to completely invalidate the previous scope, opposed to diffing the previous scope and the new scope and then removing/adding as necessary. Functionally they would result in the same thing, and in probably very comparable performance.&lt;/li>
&lt;li>Note the addition of &amp;ldquo;ingestion source&amp;rdquo;, which indicates that if we allow automated ingestion, we will need to include an ingestion source so that things don&amp;rsquo;t necessarily overwrite one another.&lt;/li>
&lt;li>This is basically just &amp;ldquo;Add or remove a batch of items&amp;rdquo; run back to back, first removing and then adding, based on a query for the ingestion source.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Remove the entire scope/blacklist for a given ingestion source
&lt;ul>
&lt;li>Similar to replace, create a query for scope from a given ingestion source and then batch remove those items.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="caveats">Caveats&lt;/h2>
&lt;p>Automating scope ingestion too frequently will wreak havoc on the PRNG, preventing us from reliably reaching full scope coverage, which was the whole point of the PRNG in the first place. This means we need to discuss mechanisms for new scope to be added without interrupting the existing PRNG cycle, but without necessarily waiting until it&amp;rsquo;s completely finished before we start scanning the new scope. One option for this is that changes to the scope go into a second scan manager, or a scan manager cache of sorts, and then jobs are distributed from between them. The problem is that this may grow out of hand if scope ingestion is happening too frequently, unless the scan manager says &amp;ldquo;Okay those are in my current scope so I&amp;rsquo;m not going to cache them, but these other ones are new so we&amp;rsquo;ll add them to our secondary job queue&amp;rdquo;. Anyways, this is just something to think about as we move towards implementing this.&lt;/p></description></item></channel></rss>